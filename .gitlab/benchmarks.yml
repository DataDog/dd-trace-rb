# -----------------------------------------------------
# Benchmarking Platform configuration
# -----------------------------------------------------

variables:
  GITLAB_BENCHMARKS_CI_IMAGE: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/benchmarking-platform:ruby-gitlab
  GITLAB_DDPROF_BENCHMARK_CI_IMAGE: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/benchmarking-platform:ruby-ddprof-benchmark
  BASE_CI_IMAGE: registry.ddbuild.io/ci/benchmarking-platform:dd-trace-rb

# -----------------------------------------------------
# Macrobenchmarks
# -----------------------------------------------------

.macrobenchmarks:
  stage: macrobenchmarks
  tags: ["runner:apm-k8s-same-cpu"]
  needs: []
  timeout: 1h
  rules:
    - if: $CI_COMMIT_REF_NAME == "master"
      when: always
      interruptible: false
    - when: manual
      interruptible: true
  # If you have a problem with Gitlab cache, see Troubleshooting section in Benchmarking Platform docs
  image: $GITLAB_BENCHMARKS_CI_IMAGE
  script:
    - git clone --branch ruby/gitlab https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/benchmarking-platform platform && cd platform
    - bp-runner bp-runner.yml --debug
  artifacts:
    name: "artifacts"
    when: always
    paths:
      - platform/artifacts/
    expire_in: 3 months
  variables:
    # Benchmark's env variables. Modify to tweak benchmark parameters.
    DD_TRACE_DEBUG: "false"
    DD_RUNTIME_METRICS_ENABLED: "true"

    DD_SERVICE: "bp-ruby-gitlab"
    DD_ENV: "staging"

    # Gitlab makes use of the rugged gem, which triggers the automatic no signals workaround use, see
    # https://docs.datadoghq.com/profiler/profiler_troubleshooting/ruby/#unexpected-failures-or-errors-from-ruby-gems-that-use-native-extensions-in-dd-trace-rb-1110
    # But in practice the endpoints we test it aren't affected, so we prefer to run the profiler in its default,
    # better accuracy, mode.
    DD_PROFILING_NO_SIGNALS_WORKAROUND_ENABLED: "false"

    # Default `false` for all features, overwrite by each variants.
    DD_TRACE_ENABLED: "false"
    DD_PROFILING_ENABLED: "false"
    DD_APPSEC_ENABLED: "false"

    ADD_TO_GEMFILE: "gem 'datadog', require: 'datadog/auto_instrument', github: 'DataDog/dd-trace-rb', ref: '$CI_COMMIT_SHA'"

    K6_OPTIONS_NORMAL_OPERATION_RATE: 30
    K6_OPTIONS_NORMAL_OPERATION_DURATION: 10m
    K6_OPTIONS_NORMAL_OPERATION_GRACEFUL_STOP: 1m
    K6_OPTIONS_NORMAL_OPERATION_PRE_ALLOCATED_VUS: 4
    K6_OPTIONS_NORMAL_OPERATION_MAX_VUS: 4

    K6_OPTIONS_HIGH_LOAD_RATE: 200
    K6_OPTIONS_HIGH_LOAD_DURATION: 5m
    K6_OPTIONS_HIGH_LOAD_GRACEFUL_STOP: 30s
    K6_OPTIONS_HIGH_LOAD_PRE_ALLOCATED_VUS: 4
    K6_OPTIONS_HIGH_LOAD_MAX_VUS: 4

    # Gitlab and BP specific env vars. Do not modify.
    KUBERNETES_SERVICE_ACCOUNT_OVERWRITE: dd-trace-rb
    FF_USE_LEGACY_KUBERNETES_EXECUTION_STRATEGY: "true"

  # Workaround: Currently we're not running the benchmarks on every PR, but GitHub still shows them as pending.
  # By marking the benchmarks as allow_failure, this should go away. (This workaround should be removed once the
  # benchmarks get changed to run on every PR)
  allow_failure: true

baseline:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: baseline

only-tracing:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: only-tracing
    DD_TRACE_ENABLED: "true"

only-profiling:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: only-profiling
    DD_PROFILING_ENABLED: "true"
    ADD_TO_GEMFILE: "gem 'datadog', github: 'datadog/dd-trace-rb', ref: '$CI_COMMIT_SHA'"

only-profiling-alloc:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: only-profiling
    DD_PROFILING_ENABLED: "true"
    DD_PROFILING_ALLOCATION_ENABLED: "true"
    ADD_TO_GEMFILE: "gem 'datadog', github: 'datadog/dd-trace-rb', ref: '$CI_COMMIT_SHA'"

only-profiling-heap:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: only-profiling
    DD_PROFILING_ENABLED: "true"
    DD_PROFILING_ALLOCATION_ENABLED: "true"
    DD_PROFILING_EXPERIMENTAL_HEAP_ENABLED: "true"
    ADD_TO_GEMFILE: "gem 'datadog', github: 'datadog/dd-trace-rb', ref: '$CI_COMMIT_SHA'"

profiling-and-tracing:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: profiling-and-tracing
    DD_TRACE_ENABLED: "true"
    DD_PROFILING_ENABLED: "true"

tracing-and-appsec:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: tracing-and-appsec
    DD_TRACE_ENABLED: "true"
    DD_APPSEC_ENABLED: "true"

profiling-and-tracing-and-appsec:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: profiling-and-tracing-and-appsec
    DD_TRACE_ENABLED: "true"
    DD_PROFILING_ENABLED: "true"
    DD_APPSEC_ENABLED: "true"

# -----------------------------------------------------
# Microbenchmarks that report to statsd
# -----------------------------------------------------

ddprof-benchmark:
  stage: microbenchmarks
  tags: ["runner:apm-k8s-same-cpu"]
  timeout: 1h
  when: manual
  image: $GITLAB_DDPROF_BENCHMARK_CI_IMAGE
  script:
    - export ARTIFACTS_DIR="$(pwd)/reports" && (mkdir "${ARTIFACTS_DIR}" || :)
    - export DD_API_KEY=$(aws ssm get-parameter --region us-east-1 --name ci.dd-trace-rb.dd_api_key --with-decryption --query "Parameter.Value" --out text)
    - git clone --branch ruby/ddprof-benchmark https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/benchmarking-platform /platform && cd /platform
    - ./steps/capture-hardware-software-info.sh
    - ./steps/run-benchmarks.sh
    - "./steps/upload-results-to-s3.sh || :"
  artifacts:
    name: "reports"
    paths:
      - reports/
    expire_in: 3 months
  variables:
    FF_USE_LEGACY_KUBERNETES_EXECUTION_STRATEGY: "true" # Important tweak for stability of benchmarks
    LATEST_COMMIT_ID: $CI_COMMIT_SHA
    KUBERNETES_SERVICE_ACCOUNT_OVERWRITE: dd-trace-rb


# -----------------------------------------------------
# Microbenchmarks
# -----------------------------------------------------

# Configuration for executing microbenchmarks in parallel
# Choose which benchmarks to execute, with how many CPUs, on what CI job
include:
  - local: benchmarks/execution.yml

microbenchmarks:
  extends: .execution  # From benchmarks/execution.yml
  stage: microbenchmarks
  when: always
  needs: []
  tags: ["runner:apm-k8s-tweaked-metal"]
  image: $BASE_CI_IMAGE
  rules:
    - if: $CI_COMMIT_REF_NAME == "master"
      interruptible: false
    - interruptible: true
  timeout: 1h
  variables:
   # GitLab CI variables passed to bp-runner
    CI_COMMIT_REF_NAME: $CI_COMMIT_REF_NAME
    CI_COMMIT_SHA: $CI_COMMIT_SHA
    CI_PROJECT_NAME: $CI_PROJECT_NAME
    CI_JOB_ID: $CI_JOB_ID
    CI_PIPELINE_ID: $CI_PIPELINE_ID
  script:
    - export ARTIFACTS_DIR="$(pwd)/artifacts" && mkdir -p "$ARTIFACTS_DIR"
    # $GROUP is defined on benchmarks/execution.yml
    - eval "export BENCHMARKS=\$BENCHMARKS_$GROUP"
    - git config --global url."https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/".insteadOf "https://github.com/DataDog/"
    - git clone --branch dd-trace-rb https://github.com/DataDog/benchmarking-platform platform && cd platform
    - bp-runner bp-runner.yml --debug
  artifacts:
    name: "artifacts"
    when: always
    paths:
      - artifacts/
    expire_in: 3 months

microbenchmarks-pr-comment:
  stage: microbenchmarks
  when: always
  needs: [microbenchmarks]
  tags: ["arch:amd64"]
  image: $BASE_CI_IMAGE
  timeout: 30m
  variables:
    CI_COMMIT_REF_NAME: $CI_COMMIT_REF_NAME
    CI_PROJECT_NAME: $CI_PROJECT_NAME
  script:
    - export ARTIFACTS_DIR="$(pwd)/artifacts" && mkdir -p "$ARTIFACTS_DIR"
    - git config --global url."https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/".insteadOf "https://github.com/DataDog/"
    - git clone --branch dd-trace-rb https://github.com/DataDog/benchmarking-platform platform && cd platform
    - bp-runner bp-runner.pr-comment.yml --debug
  artifacts:
    name: "artifacts"
    when: always
    paths:
      - artifacts/
    expire_in: 3 months
